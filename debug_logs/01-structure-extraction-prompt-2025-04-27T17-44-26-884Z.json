[
  {
    "role": "system",
    "content": "You are a scientific paper structure analyzer that extracts the document organization precisely."
  },
  {
    "role": "user",
    "content": "Extract the structure of this scientific paper, identifying:\n1. The paper title\n2. The abstract\n3. All sections with their paragraphs\n\nReturn a JSON object with this structure:\n{\n  \"title\": \"The paper title\",\n  \"abstract\": \"The full abstract text\",\n  \"sections\": [\n    {\n      \"name\": \"Section name\",\n      \"paragraphs\": [\n        \"Full text of paragraph 1\",\n        \"Full text of paragraph 2\",\n        ...\n      ]\n    },\n    ...\n  ]\n}\n\nPAPER TEXT:\nMisleading causal statements in functional connectivity research\r\n\r\n\r\n\r\n\r\n\r\nDavid Marc Anton Mehler1, 2 and Konrad Paul Kording3, 4\r\n\r\n1 Cardiff University Brain Research imaging Centre (CUBRIC), School of Psychology, and MRC Centre for Neuropsychiatric Genetics and Genomics, Division of Psychological Medicine and Clinical Neurosciences, School of Medicine, Cardiff University, Cardiff CF244HQ, UK\r\n\r\n2 Department of Psychiatry, University of Münster, Germany\r\n\r\n3 Department of Bioengineering and Department of Neuroscience, University of Pennsylvania, USA\r\n\r\n4 Canadian Institute for advanced Research (CIFAR)\r\n\r\n\r\n\r\nAbstract: \r\n\r\nAs neuroscientists we want to understand how causal interactions and mechanisms within the brain give rise to perception, cognition, and behavior. It has become popular to estimate interactions from measured activity using statistical techniques that aim to estimate “functional connectivity”, and interpret these correlational estimates in causal terms. However, we argue that any given observed correlation can be explained by an infinite set of causal models that consider the unobserved variables. We show that the omitted variable bias equation makes this relation precise: as we only record low-dimensional signals from high-dimensional brains, the countless unobserved variables likely have more influence than the few observed ones. We argue that only brain perturbations allow approaching causality. \r\n\r\n \r\n\r\nKeywords: functional connectivity, causality, omitted variable bias, neuroscience, Simpson’s paradox\r\n\r\nHighlights \r\n\r\nCausal inference usually requires well-controlled perturbations. The neurosciences offer a continuum of approaches that allow establishing causality. \r\n\r\nFunctional connectivity (FC) estimates obtained without perturbations based on (potentially nonlinear) correlations are often wrongly interpreted as establishing causal and mechanistic effects – so much so that they often drive clinical stimulation studies. \r\n\r\nWe can understand why correlations do not even approximate causation from a consideration of the effect of the unmeasured variables, the so-called omitted variable biases.\r\n\r\nThe language used by the field massively and misleadingly suggests causal or mechanistic interactions, much to the detriment of progress in neuroscience.\r\n\r\n\r\n\r\nThe estimated connectivity field does attempt to draw causal insights from correlations\r\n\r\nFunctional connectivity research has become a substantial part of neuroscientific literature in recent years [1]. Typically studies uses observational methods to estimate “connectivity” via statistical techniques that aim to infer interactions from continuous valued brain signals [2,3]. The approaches that we call estimated connectivity (eC) in this paper convert measured signals into a statistical estimate of “connectivity.” \r\n\r\nWe argue that the eC field uses language that suggests causality and its results aim to drive downstream research as if it was causality. Terms used to describe its methodology include Granger Causality, functional connectivity (FC), information flow, effective connectivity and dynamic causal models [3]. The field does this despite the fact that other terms such as improvement in predictive power, correlations, conditional correlations, and model comparison can denote more precisely what is actually done. The language effectively redefines causality related terms in the English language. Connectivity implies a connection between two places. Causality implies cause and effect. Flow implies that something moves from one place to another. This set of re-definitions gives rise to the problem that eC approaches are often misunderstood by practicing neuroscientists [4]. \r\n\r\nThis misleading redefinition profoundly affects downstream science. Scientists write about connections within the brain minimizing the wiring length along which signals need to travel [5,6], but while the brain may want to minimize wiring length, it pays no price for correlations. They write about stimulation to control the network [7], which requires interactions to be causal. They write about “using causal FC methods to identify network mechanisms underlying cognitive computations in the human brain” [8], which implies that prediction is a sufficient (and not just necessary) criterion to justify causal inferences. They write about mechanistic interference to cure diseases [9], which again requires causality. Or they write about regions that “cause more exchange of causal information” [10]. These examples show how correlations are assumed to indicate causality. Specifically, eC is often discussed as if it did reveal an approximate understanding of causality, and much of it is due to misleading use of words that imply causality in lay English and merely refer to algorithm properties in statistics. \r\n\r\nWe ourselves fell for this interpretation. We wrote in 2008  “ […] [estimated connectivity] methods have become staples of neural data analysis, and have revealed a great deal about the interactions between cortical and subcortical structures.” [11]. We could simply have said that models that use other activities as independent variables are good predictors.  With LFPs it was argued that “[…] the relative weight of feedforward and lateral inputs in visual cortex is not fixed, but rather depends on stimulus contrast.” [12]. For EEG and MEG power coherence analyses, it was advocated that “[…] amplitude correlation is an informative index of the large-scale cortical interactions that mediate cognition.” [13]. For fMRI data, other authors reported “[…] changes in the architecture of functional connectivity patterns that promote learning from initial training through mastery of a simple motor skill.” [14]. However, all these approaches only reach level one in Pearl’s hierarchy of causation [15,16] – they describe correlations. While the impossibility of making causal statements in such situations is well known [17,18], the field continues to make them. Indeed, a recent perspective paper by published by leading researchers in the eC field explicitly states that it strives causality, predominantly by refining eC measures [2]. \r\n\r\nAlgorithms claiming eC have been used exhaustively when analyzing brain data with the hope of getting at causality. Using a more accurate terminology could help in the interpretation and, clarify that our results remain foremost descriptive: Interregional, or interneural signal correlations captures what most techniques measure. \r\n\r\n\r\n\r\n\r\n\r\nDefining causality\r\n\r\nIt has been argued that the definition of causality in neuroscience is ambiguous [4,19]. Whenever we refer to causal interactions in this opinion paper, we will use the counterfactual definition. A variable causally influences another variable if a perturbation of the first variable would induce a change in the activity of another variable [15,20]. This approximates what we mean as neuroscientists: if we say a neuron influences another neuron, we mean that perturbing the first (e.g. electrically) would affect the second and if we say that a region influences another, we mean that perturbing the first region (e.g. magnetically) would affect the second. Hence, causality has an intuitively clear definition (counterfactuals or perturbation) and we should demand our statistical approaches to be measured against it (although see also Gomez-Marin, 2017). While there is significant debate about adequate definitions of causality in contemporary philosophical literature [22–25], we argue that neuroscientists by and large use our definition and will underscore this point with the way these scientists make claims in their papers. After all, this interpretation is what allows making claims about mechanism and clinical targets. \r\n\r\n\r\n\r\n\r\n\r\nBox 1. Statistical methods to estimate “connectivity”\r\n\r\nA rich body of literature has described eC techniques: some techniques utilize granger causality [26], other techniques talk about information flow [27,28]. Another class more directly talks about Dynamic Causal Modeling [29]. Within the neuroimaging community the term effective connectivity (EC) is often used when causality is explicitly claimed but within our definition, they fall into our more expansive definition of eC. We will argue that all these approaches share the same logical weakness – statistical confounding. This problem is often acknowledged by the statisticians developing these algorithms and yet generally ignored by the experimentalists using them. Yet, the lure of extracting causality from observational data is so powerful, that we cannot avoid feeling the pull of it and have ourselves referred to correlations in connectivity terms [11]. \r\n\r\nThe field has also developed techniques for estimating the strength of connections between neurons based on simultaneous spike recordings. The underlying idea is that we want to predict each neuron’s spiking probability based on the activity of other observed neurons, typically expressed in form of mechanistic claims [30,31]. And indeed, if we record all neurons, they are noisy, and we know that from their transfer function we should be able to estimate the strength and nature of causal interactions [32]. These studies, too, were generally interpreted in causal terms [11,30]. Again, the lure of causality is so powerful that it affects interpretations.\r\n\r\nWithin the eC community one can delineate two main philosophies. Some authors use eC to describe replicable network properties of functional and anatomical neural data without directly attributing causal significance to interregional correlations [33–35]. Others interpret changes in parameter estimates in more explicitly causal terms such as contributing factors of pathophysiological processes [36,37], biological mechanisms in learning and cognition in healthy [38,39], as vulnerability mechanisms to develop certain disorders [40,41], or in terms neuroplasticity in the functional re-organization of the brain [37,42]. Common to both groups is the focus on making mechanistic claims. Indeed, recent bibliometric research suggests that the study of eC has become a substantial area in neuroimaging [1], and it is thus is important to obtain clarity about its interpretation. We argue, however, that neuroscientists systematically interpret their correlational findings as causal by the misleading use of words that suggest mechanisms and causality. \r\n\r\n\r\n\r\nConfounding\r\n\r\nThe confounding problem is the existence of unobserved variables that affect the observed variables (both a potential upstream population A and a potential downstream population B) in a way that generally makes the estimation of causal effects impossible. It is easy to see why it is impossible to use any statistical techniques to estimate causal interactions in the presence of unobserved confounding. Let us say there is a causal influence from A to B. In this case, it is always possible to construct a common input C which will produce the same effect on B that A would have (e.g. by replicating A and feeding into C). Similarly, if there is no influence from A to B it is always possible to use a confounder to change A and B so that they now look like they do interact. Clearly there is an infinite set of potential causal interpretations of the same partially observed signals.\r\n\r\nFor example, let us say we are interested in the interactions between two neurons. Further, let us assume that there is a third neuron (confounder) that activates both neurons. In that case, if we see a correlation between the neurons we cannot know if it is due to the neurons interacting directly or through an induced interaction by the unobserved neuron. More generally, any joint distribution between two neurons could be induced by a single third neuron. Unobserved neurons or information can act as confounders for the inference of eC, threatening the validity of the results. This confounding problem is central to the statistical field of causal inference and it is generally acknowledged that, in typical situations, unobserved confounders render inferences about causality impossible. \r\n\r\nWe want to use Simpson's paradox to highlight the threat of confounding [43], in which unobserved interaction can either cancel out main effects, or artificially induce spurious main effects. Let us say that there are two brain states (e.g. related to two levels of attention), and that one state is associated with high activity of neuron A and low activity of neuron B while another is associated with low activity of neuron A and high activity of neuron B. But let us say that there is a positive instantaneous causal influence from one on the other. If we do not know the brain state (confounded) we may conclude that neuron A has a negative influence on the activity of neuron B. However, if we do know the brain state, we correctly see that A increases the activity of neuron B (Figure 1). This paradox shows how confounders with relevant structure can arbitrarily influence the resulting eC and can effectively destroy it.  \r\n\r\nIndeed, in certain neuroimaging datasets it is possible to directly observe Simpson’s paradox. For instance, the choice of analysis parameters, such as seed regions, and the network they belong to, may mediate whether different FC methods yield similar, or orthogonal results [44]. Moreover, functional brain parcellations reconfigure based on the brain state [45]. \r\n\r\nThese issues would maybe be a minor problem if we had reasons to believe that confounding is weak, i.e. if there were few unobserved confounders and that they would be weak. However, we will demonstrate that we have no reason to believe so, and that the number of such confounders is in fact larger by orders of magnitude (and their strength arguably roughly the same) than the observed signals. We will argue, based on the omitted variable bias equation (Box 2) that it is highly unlikely that an algorithm could identify the network of causal interactions. \r\n\r\n\r\n\r\n\r\n\r\nFigure 1. A) Simpson’s paradox illustrated for the simplest two-neuron case.\tThe red dots represent neural activity recordings of neuron A and B. The yellow line indicates a negative correlation between activity of neuron A and B. However, this correlation is confounded by two different brain states. The blue lines indicate that within each brain state there is a positive correlation between the activity of neuron A and B. To unravel this positive correlation, knowledge about the different brain states is required.  Simpson’s paradox shows that the sign of a correlation is not indicative of causality. B) Simpson’s paradox illustrated for BOLD time series of two fMRI voxels, based on [44] . The left panel shows fictional time series of two voxels which are negatively correlated in three individuals. The right panel shows the mean values for these time series, which are positively correlated across participants. BOLD = Blood oxygenation level dependent. \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nThe omitted variable bias (OVB)\r\n\r\nNeuroimaging datasets (e.g. fMRI and MEG) are affected by obvious confounders including head motion and physiological processes. One dominant view is that correcting for obvious confounders improves causal inference irrespective of unobserved confounders [2]. However, for each obvious external controllable confounder there are countless internal unobservable ones [46]. Hence, while we agree that correcting for observed confounders improves the prediction of models, we argue that such view remains speculative when it comes to inferring causality for theoretical reasons that we lay out below. \r\n\r\nFor better illustration, we refer to a linear system (see Box 2). However, we note that OVB equally applies to nonlinear systems [47,48] as, the existence of unobserved variables introduces a bias that persists, regardless the amount of data. Given the nature of recorded brain signals, there is no reason to believe that the observed variables (e.g. average activation in a voxel) should have more causal influence than the unobserved variables (e.g. any represented variable orthogonal to this). Signals recorded in neuroscience usually entail millions more unobserved signals than observed signals [49,50]. Hence, the unobserved noise is expected to be arbitrarily larger than the signal obtained from observed neural activity. Therefore, statistical techniques will fail to reveal causal insights from correlations unless we have strong additional prior knowledge about the system and its dynamics. \r\n\r\n\r\n\r\nBox 2. Mathematical demonstration of the omitted variable bias \r\n\r\nLet us assume that there is a set of variables x that are observed. We want to ask what the causal influence of x is on y. Both x and y may be neural activities that relate in a linear way to each other: \r\n\r\n+z𝛿 \t\t\t\t\t\t\t\t\t(1)\r\n\r\nwhere  is the weight for the influence that signal x has on y. Further, 𝛿 is the weight of the noise term z, which includes other variables that we do not observe but that also have a causal influence on x and y. These unobserved variables introduce OVB onto the causal estimate of the influence of x and y. Importantly, OVB cannot be overcome by more data, it exists in the limit of infinite sized datasets. Further, following a simple back of the envelope calculation for the relevant effects studied in the FC and EC fields, OVB should, be orders of magnitude larger than the real signals, entirely rendering the techniques useless for their typical applications.\r\n\r\nWhen we apply a linear regression model, we typically ignore z and calculate as a mean square error estimate: \r\n\r\n\t\t\t\t\t\t\t\t(2)\r\n\r\nHowever, once we take the noise (z) into account, we can insert the equation for y (1) into the equation for :\r\n\r\n\t\t\t\t\t\t\t(3)\r\n\r\nIn other words, the existence of unobserved variables introduces a bias that persists, regardless the amount of data (X). There is no reason to believe that the  contribute more variance than the . The correlations between x and z are typically relatively high [51]. In fact, neural signals recorded in neuroscience usually entail millions more unobserved signals than observed signals [49]. Hence, the signal from the z term is expected to be arbitrarily larger than the signal from x and statistical techniques are highly susceptible to OVB when applied to merely observational data.  \r\n\r\nAs the neural dimensionality (or speed of processing) in each measured signal increases, the temporal resolution decreases, the noise (or non-communication related signal) increases and hence the idea of extracting causality from observation becomes hopeless. Importantly, this is not a problem that can simply be solved by recording data from more subjects. This is a case where a problem can fundamentally not be solved.\r\n\r\n\t \r\n\r\nThe number of confounders is astronomical \r\n\r\nOmitted variable biases will make causal inference impossible if there are many unobserved dimensions relative to the observed dimensions. For instance, when analyzing spike data, there are far more unobserved variables than observed variables. When we record a few hundred neurons [52], the number of recorded neurons is a vanishingly small subset of all neurons. We have little reason to assume that the recorded neurons are much more important than the un-recorded neurons. As each neuron receives inputs from so many other un-recorded neurons, we should expect that the parts of neural activity driven by unobserved neurons are arbitrarily larger than the parts coming from observed neurons. In other words, the confounding signal should be many orders of magnitude more important than those coming from observed data. As such, we should not expect that causal inference is possible. \r\n\r\nWe might feel that causality may happen at a given scale, which would make the argument about unobserved dimensions invalid and allowing a multi-scale definition of interactions. The argument given is often the analogy to statistical physics: while understanding the interaction between individual gas molecules becomes quickly hopeless, a large set of gas atoms can be well understood in terms of temperature and pressure. However, this analogy quickly breaks down when applied to neurophysiology. Every neuron is special in the sense that they do not interact with random neurons, but with a largely fixed set, or worse, a set that changes through neuroplasticity. The justification of averaging over molecules is often perfectly fine in statistical physics. However, whether this this logic works in neuroscience remains an open question. \r\n\r\nWhen analyzing imaging data such as fMRI, or LFP, EEG, or MEG, there are also far more unobserved variables than observed variables. Within each signal source, we can, in some abstraction, observe the sum of neural activity. However, the same measured activity can be realized by any combination of individual activities rendering a solution of the inverse problem (signals 🡪 neuronal spike trains) infeasible. The activity of neurons which are orthogonal to our signal, can span arbitrary dimensions, related to movement, memory, thought or neuronal communication. Importantly, dense physiological recordings in small areas suggest that countless variables are represented (e.g. movement related signals in V1; Musall et al., 2018; Stringer et al., 2018). The signals that we can measure are arbitrarily low-dimensional relative to the full dimensionality of communication in the brain. As such we are still in the situation where we have a number of confounders that is many orders of magnitude larger than the number of measured variables. This again puts us into the domain where causal inference should be impossible. \r\n\r\nMacroscopic recording (such as fMRI) is what, to many people, hides the underlying logical problems of confounding. We record from a number of voxels or pixels. In many cases, we cover the whole brain. So where do the confounders hide? Within each voxel there are generally millions of neurons that form billions of synapses [50]. However, we only observe the macroscopic activities, say the sum of all activities. This means that all the dimensions that are uncorrelated to this sum continue to exist and continue to be transmitted to other voxels. In other words, there is nothing about the setting that removes confounders, unless the voxel activity itself is the confounder. Given that the majority of dimensions is unobserved, the probability of recording the confounder is vanishingly small. \r\n\r\nIt has been argued that statistical approaches may still uncover low-dimensional organization between brain regions [2]. The argument is that if the brain’s activity was very low dimensional and aligned with recorded dimensions [55], then recording from a small number of voxels or neurons may be equivalent to recording all of them. Such view assumes that we believe that the composition of the activity within a voxel, for instance, does not matter but just the measured sum of (unobserved) individual activities. We thereby effectively subscribe to a strong mean-field view, which is dominant among eC methodology [56–59]. However, within each voxel, we find neurons of countless tuning properties [60] that describe how neurons are affected by a stimulus dimension such as color. On a theoretical level, it thus becomes clear how mean-field or ensemble averaging techniques fall risk of violating ergodicity principles as a key assumption of cause-effect inference, i.e. that the mean response of representative samples allow predictions about individual members of those samples [61]. On a practical level, model inferences becomes easily biased when algorithms that compute eC based on mean-field variables (e.g. the average activity within several voxels informed by some parcellation)  miss relevant nodes [45,51]. Lastly we argue that effectively, mean-field studies set up simulations that unfold in low-dimensional spaces and then show that they can be recovered in low-dimensional spaces, rendering the approach somewhat circular [56–59]. To our knowledge no study has shown that networks that compute like brains can be approximated by the underlying mean-field approximation. Barring such a study we should assume that mean-field is not a meaningful approximation of brain computation. \r\n\r\nTypically applied eC algorithms are unable to disentangle correlational from causal sources of variability regardless their mathematical sophistication [3,61]. Causal inference algorithms that work with observational data are generally built on the assumption of causal sufficiency, which boils down to there being no unobserved confounders (although see Ranganath and Perotte, 2018; Wang and Blei, 2018) – this assumption within neuroscience is that all neurons are recorded from. Without these assumptions we can at best produce families of potential models and if any pair of recorded variables is confounded then this family will contain all models [64]. Recording only a few variables in a densely interacting causal system generally renders causal inference impossible [20,65]. \r\n\r\n\r\n\r\n\r\n\r\nWhy higher resolution is not the solution\r\n\r\nWe may ask if causal inference may work for exhaustively recorded (i.e., completely observed) neural networks, for instance in small animals or engineered systems. In the worm c. elegans, aplysia, drosophila, larval zebrafish [66] or microprocessors [67] recording all “neurons” at high temporal precision is possible. For instance, in very small simulated systems that contain only a few interconnected neurons we may find that correlation approaches true, causal connectivity (https://compneuro.neuromatch.io/tutorials/W3D5_NetworkCausality/student/W3D5_Tutorial2.html). However, once the network becomes larger (or, equivalently, there being stronger spatial autocorrelation, estimated and true connectivity diverge, and model identification becomes impossible (Figure 2). Likewise, model inference based on merely observational data (i.e. network correlations) becomes impossible in larger, interconnected systems. This is because mismatches between the generative system and the inference model (i.e. model mis-specification) become inevitable , and highly correlated, yet not directly coupled activity (as typically observed in recurrent networks) will be easily mistaken for direct connections [51]. This underdetermination (or “missing region” problem and the bias it introduces is well known [29,51] and only controlled perturbations can prevent such systematic inference errors   [51,68]. \r\n\r\nAs the impossibility of causal inference from subsampled data may feel counter-intuitive we want to spell out the problem a bit more. Let us assume we are interested in the connections between two signals, e.g. voxels B and C. However, we note the brain’s real dynamics are characterized by the activity of up to more than 5 million neurons contained in these voxels [50]. In its simplest form, these can be modelled as a linear system that includes a noise term for unexplained variance. As the neural dimensionality (or speed of processing) in each measured signal increases, the noise (or non-communication related signal) increases and hence the idea of extracting causality from observation becomes hopeless. Importantly, this is not a problem that can simply be solved by recording data from more observations. Increasing data can only decrease variance errors but not bias errors, which result from correlated neural activities that are not averaged out by merely increasing data [51]. Bias errors will persist unless perturbations are applied [51].  \r\n\r\nFor all methods that we have at our disposal to run functional connectivity analyses, the signals are few relative to the many unobserved variables. In these settings, interactions between measured signals should describe much less variance than the interactions between unobserved variables. While the examples we discuss here focus on neuron-to-neuron interactions, additional complexity arises from interactions with other cell types such as glia. Just like in Simpson’s paradox, the interactions between such unobserved variables can arbitrarily affect the estimates of interactions between measured signals [49]. As the neural dimensionality in each measured signal increases, the noise (or non-communication related signal) increases and interactions between unobserved variables can arbitrarily affect the estimates of interactions between measured signals, resulting in OVB [68]. Hence, all reviewed statistical methods are ultimately susceptible to OVB [48,69,70]. \r\n\r\n\r\n\r\n\r\n\r\nFigure 2: In high dimensional systems correlations are very different from correlation. A) The left panel shows a true, deterministic connectivity matrix A, the panel the estimated connectivity matric R. B) The curve shows the similarity (i.e. frobenius norm) between the true and estimated connectivity matrices for simulated simple systems of varying dimensionality and matched largest singular value. As the system becomes larger, dimensions increase (or, equivalently, there being stronger spatial autocorrelation), estimated connectivity strengths capture less true connectivity and we fail to identify the true connectivity model.  Replotted from Neuromatch Academy (https://compneuro.neuromatch.io/tutorials/W3D5_NetworkCausality/student/W3D5_Tutorial2.html) [71,72]. \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nCorrect “estimated connectivity” from perturbations\r\n\r\nA continuum of perturbation approaches to study causal interactions in brain activity are available in the neurosciences that should motivate future developments [19]. Counterfactual, causal inference ideas can be used to assess the strength of \tevidence about causality that can be obtained with commonly used approaches. Considerations of the Bradford-Hill criteria, provide a starting point for future research on causality in neuroscience. These perturbation studies also come with limitations, e.g. sources of confounding and still limited understanding of involved mechanisms [73,74]. Istill  for those studies However, perturbation and in particular (passive) brain stimulation techniques are currently as close as we can get to causality in neuroscience when employed in  a randomized, adequately controlled and blinded way [75]. \r\n\r\nEmerging techniques for perturbations may confront us with new conceptual problems. Perturbation techniques such as neurofeedback training and brain computer interfaces (BCIs) may be seen as an approximation to direct perturbations [79], because they allow to entrain correlations between brain regions and test for desired behavioral changes and have thus been advocated as  tools for trying to understand causality within the brain [80–82]. BCIs synchronized in multiple users (so called hyperscanning) provide a special use case that allows closed-loop control of neural synchrony. These developments have  recently  initiated debates about causal inferences that can be drawn from well-controlled BCI experiments [83–86]. As complex interventions BCIs entail various obvious confounders and require experiments with (often many) carefully designed control conditions [87]. Combining BCIs with brain stimulation techniques may circumvent some limitations of other approaches. Further, complementary invasive closed-loop optogenetic stimulation studies conducted in animals, where stimulation is triggered by learned neural activity patterns [88], may  circumvent some of the confounders and provide converging evidence [19,89]. Conceptual work is needed, however, to obtain clarity about these interpretations. \r\n\r\nLastly, clinical applications often use eC measures, not to quantify causal interactions but as a proxy for hard-to measure behavioral effects. Clinicians ask if interventions affect eC as “biomarkers”. Brain stimulation techniques can thus be “eC-navigated”, i.e. targets are defined based on eC measures that predict (individual) clinical outcomes [90]. Brain Computer interfaces (BCIs) can target eC measures using eC changes as readily optimizable targets [83,91]. eC may be the best quantitative biomarkers for intervention effects when tested in adequately designed randomized controlled trials. They may also enable a quest to understand if low-dimensional organization principles can approximate causality [2]. However, research based on eC biomarkers remain somewhat circular field where changes in low-dimensional space are used to justify low-dimensional assumptions (e.g. mean-field based eC between two brain regions). Thus, causal insight about eC biomarkers (e.g. when used in eC-navigated perturbations) will require converging evidence [19] across organisational levels of the nervous system (i.e. molecular mechanisms) [49] and, above all, tests based on perturbations. \r\n\r\nConcluding remarks\r\n\r\nWe have reviewed why eC approaches cannot meaningfully get at causality due to massive confounding from unobserved variables – correlations are not causation and omitted variables produce biases. However, this circumstance does not imply that trying to understand the joint dynamics of many neurons or brain areas is not interesting. For example, looking at the brain in lower dimensional projections may allow us to see its invariances [92–94]. Interregional correlations may have no causal meaning, but they may allow us to derive biomarkers [95–97]. To which degree these reflect neuronal interactions, however, remains subject to future perturbational, well controlled studies. Understanding joint statistics of neurons or brain networks is highly informative, however, statements about joint statistics are not meaningful statements about causality – it is important to be clear about the statements permitted by any one approach. In cases where these cannot be substantiated, only descriptive approaches [98] non-causal explanations are informative and meaningful [25,99]. Lastly, we note that there may be also a sociological dimension to the raised issues: authors often seem incited to make causal statements using filler verbs (e.g. to drive, alter, promote) as a form of storytelling [21] to leverage the perceived attractiveness of their work. Instead of committing this fallacy, we encourage researchers to question the causal validity of statements, probe underlying assumptions and test counterfactual inference boundaries (see Outstanding questions). \r\n\r\n\r\n\r\nOutstanding questions \r\n\r\nWhat is an adequate nomenclature for causality inferences in neuroscience that reflects the different degree of uncertainty associated with individual approaches? \r\n\r\nFor which questions do we need causality? For instance, clinical research may find biomarkers that indicate a disease state but are not part of the causal chain or about which we have little mechanistic insight. Yet, they may be suitable to predict disease trajectories or treatment response. \r\n\r\nWhat are the counterfactual inference boundaries for perturbation approaches and how can these be considered with adequate analyses? There are several approaches that may be useful, for instance tests and simulation studies that investigate under which circumstances there is likely confounding by Simpson’s paradox. In neural systems that are fully described, targeted perturbations can be used to probe whether algorithms can reconstruct expected patterns. \r\n\r\nWhich mechanisms underly effects of perturbational approaches such as brain stimulations and BCIs? How can study protocols for combined perturbational approaches be optimized to yield converging causal mapping?\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nAcknowledgements\r\n\r\nWe would like to thank our colleagues for insightful comments and discussions that have substantially contributed to the content of this manuscript: Alex Gomez Martin, Anil Seth, Brian Collopy, Daniele Marinazzo, David Linden, Dimitris Pinotsis, Ian Stevenson, Jean Daunizeau, Johannes Algermissen, Jonathan Pillow, Juan Alvaro Galego, Karl Friston, Klaas Enno Stephan, Lionel Barnett, Manjari Narayan, Michael Haufe, Robert Kass, Roscoe Brady, Russ Poldrack and Stefan Rotter (in alphabetical order). \r\n\r\n\r\n\r\nData availability statement\r\n\r\nData and code available on: https://osf.io/9cs8p/ \r\n\r\n\r\n\r\nConflict of interest\r\n\r\nDMAM has received payments to consult with a neurofeedback start-up company. KPK has no conflict of interest to declare. \r\n\r\n\r\n\r\n\r\n\r\nReferences\r\n\r\n1 \tDworkin, J.D. et al. (2018) The landscape of NeuroImage-ing research. Neuroimage 183, 872–883\r\n\r\n2 \tReid, A.T. et al. (2019) Advancing functional connectivity research from association to causation. Nat. Neurosci. 22, 1751–1760\r\n\r\n3 \tSemedo, D. et al. (2020) Statistical methods for dissecting interactions between brain areas. Curr. Opin. Neurobiol. DOI: 10.1016/j.conb.2020.09.009\r\n\r\n4 \tBarack, D.L. et al. (2022) A call for more clarity around causality in neuroscience. Trends Neurosci. DOI: 10.1016/j.tins.2022.06.003\r\n\r\n5 \tBullmore, E.T. and Bassett, D.S. (2011) Brain Graphs: Graphical Models of the Human Brain Connectome. Annu. Rev. Clin. Psychol. 7, 113–140\r\n\r\n6 \tSorrentino, P. et al. (2021) The structural connectome constrains fast brain dynamics. Elife 10, 1–11\r\n\r\n7 \tTaylor, P.N. et al. (2015) Optimal control based seizure abatement using patient derived connectivity. Front. Neurosci. 9, 1–10\r\n\r\n8 \tSanchez-Romero, R. et al. (2021) Causally informed activity flow models provide mechanistic insight into the emergence of cognitive processes from brain network interactions. bioRxiv DOI: 10.1101/2021.04.16.440226\r\n\r\n9 \tKhambhati, A.N. et al. (2016) Virtual Cortical Resection Reveals Push-Pull Network Control Preceding Seizure Evolution. Neuron 91, 1170–1182\r\n\r\n10 \tBajaj, S. et al. (2015) Brain effective connectivity during motor-imagery and execution following stroke and rehabilitation. NeuroImage Clin. 8, 572–582\r\n\r\n11 \tStevenson, I.H. et al. (2008) Inferring functional connections between neurons. Curr. Opin. Neurobiol. 18, 582–588\r\n\r\n12 \tNauhaus, I. et al. (2009) Stimulus contrast modulates functional connectivity in visual cortex. Nat. Neurosci. 12, 70–76\r\n\r\n13 \tSiegel, M. et al. (2012) Spectral fingerprints of large-scale neuronal interactions. Nat. Rev. Neurosci. 13, 121–134\r\n\r\n14 \tBassett, D.S. et al. (2015) Learning-induced autonomy of sensorimotor systems. Nat. Neurosci. 18, 744–751\r\n\r\n15 \tPearl, J. and Mackenzie, D. (2018) The book of why. The new science of cause and effect., (1st edn) Basic Books.\r\n\r\n16 \tDablander, F. (2019) , An introduction to Causal inference. . [Online]. Available: https://fabiandablander.com/r/Causal-Inference.html\r\n\r\n17 \tHolland, P.W. (2015) Causal Inference , Path Analysis , and Recursive Structural Equations Models. 18, 449–484\r\n\r\n18 \tDawid, A.P. (2008) Beware of the DAG! NIPS 2008Workshop Causality DOI: 10.2459/JCM.0b013e32833dadd5\r\n\r\n19 \tSiddiqi, S.H. et al. (2022) Causal mapping of human brain function. Nat. Rev. Neurosci. 23, 361–375\r\n\r\n20 \tPearl, J. (2009) Causality: Models, Reasoning and Inference, (2nd edn) Cambridge University Press.\r\n\r\n21 \tGomez-Marin, A. (2017) Causal Circuit Explanations of Behavior: Are Necessity and Sufficiency Necessary and Sufficient? Decod. Neural Circuit Struct. Funct. DOI: 10.1007/978-3-319-57363-2_11\r\n\r\n22 \tAriano, G.M.D. et al. (2018) Causality re-established. Philos. Trans. R. Soc. A \r\n\r\n23 \tBastit, M. (2014) Causality : From Contemporary Scientiic Conceptions of Causality to the Classical Four Causes. Angelicum 91, 37–58\r\n\r\n24 \tNorton, J.D. and Norton, J.D. (2009) Is There an Independent Principle of Causality in Physics ? Br. Soc. Philos. Sci. 60, 475–486\r\n\r\n25 \tM. Chirimuuta (2018) Explanation in Computational Neuroscience: Causal and Non-causal. Br. J. Philos. Sci. 69, \r\n\r\n26 \tBressler, S.L. and Seth, A.K. (2011) Wiener-Granger Causality: A well established methodology. Neuroimage 58, 323–329\r\n\r\n27 \tHoney, C.J. et al. (2007) Network structure of cerebral cortex shapes functional connectivity on multiple time scales. Proc. Natl. Acad. Sci. 104, 10240–10245\r\n\r\n28 \tBabiloni, F. et al. (2005) Estimation of the cortical functional connectivity with the multimodal integration of high-resolution EEG and fMRI data by directed transfer function. Neuroimage 24, 118–131\r\n\r\n29 \tDaunizeau, J. et al. (2011) Dynamic causal modelling: A critical review of the biophysical and statistical foundations. Neuroimage 58, 312–322\r\n\r\n30 \tPillow, J.W. et al. (2008) Spatio-temporal correlations and visual signalling in a complete neuronal population. Nature 454, 995–999\r\n\r\n31 \tStevenson, I.H. et al. (2012) Functional Connectivity and Tuning Curves in Populations of Simultaneously Recorded Neurons. PLoS Comput. Biol. 8, \r\n\r\n32 \tKarbasi, A. et al. (2018) Learning neural connectivity from firing activity: efficient algorithms with provable guarantees on topology. J. Comput. Neurosci. DOI: 10.1007/s10827-018-0678-8\r\n\r\n33 \tSmith, S.M. et al. (2009) Correspondence of the brain’s functional architecture during activation and rest. Proc. Natl. Acad. Sci. 106, 13040–13045\r\n\r\n34 \tYeo, B.T.T. et al. (2011) The organization of the human cerebral cortex estimates by intrinsic functional connectivity. J. Neurophysiol. DOI: 10.1152/jn.00338.2011.\r\n\r\n35 \tRaichle, M.E. et al. (2001) A default mode of brain function. Proc. Natl. Acad. Sci. 98, 676–682\r\n\r\n36 \tEdlow, B.L. et al. (2021) Recovery from disorders of consciousness: mechanisms, prognosis and emerging therapies. Nat. Rev. Neurol. 17, 135–156\r\n\r\n37 \tPrice, R.B. and Duman, R. (2020) Neuroplasticity in cognitive and psychological mechanisms of depression: an integrative model. Mol. Psychiatry 25, 530–543\r\n\r\n38 \tFarahani, F. V. et al. (2019) Application of graph theory for identifying connectivity patterns in human brain networks: A systematic review. Front. Neurosci. 13, 1–27\r\n\r\n39 \tCohen, J.R. (2018) The behavioral and cognitive relevance of time-varying, dynamic changes in functional connectivity. Neuroimage 180, 515–525\r\n\r\n40 \tKirsch, D.E. et al. (2021) Neural Functional Connectivity Changes to Psychosocial Stress in Young Adults with Bipolar Disorder and Preliminary Associations with Clinical Trajectories. Bipolar Disord. 24, 298–309\r\n\r\n41 \tCiarrusta, J. et al. (2020) Emerging functional connectivity differences in newborn infants vulnerable to autism spectrum disorders. Transl. Psychiatry DOI: 10.1038/s41398-020-0805-y\r\n\r\n42 \tHallam, G.P. et al. (2018) Task-based and resting-state fMRI reveal compensatory network changes following damage to left inferior frontal gyrus. Cortex 99, 150–165\r\n\r\n43 \tSimpson, E.H. (1951) The interpretation of interaction in contigency tables. J. R. Stat. Soc. Ser. B 13, 238–241\r\n\r\n44 \tRoberts, R.P. et al. (2016) The Simpson’s paradox and fMRI: Similarities and differences between functional connectivity measures derived from within-subject and across-subject correlations. Neuroimage 135, 1–15\r\n\r\n45 \tSalehi, M. et al. (2019) There is no single functional atlas even for a single individual: Functional parcel definitions change with task. Neuroimage 208, 116366\r\n\r\n46 \tDiDomenico, R. and Eaton, R. (1988) Seven Principles for Command and the Neural Causation of Behavior. Brain. Behav. Evol. 31, 125–140\r\n\r\n47 \tHoyer, P.O. et al. (2009) Nonlinear causal discovery with additive noise models. Adv. Neural Inf. Process. Syst. 21, 689–696\r\n\r\n48 \tStevenson, I.H. (2018) Omitted Variable Bias in GLMs of Neural Spking Activity. Neural Comput. 30, 1–32\r\n\r\n49 \tUrai, A.E. et al. (2022) Large-scale neural recordings call for new insights to link brain and behavior. 25, \r\n\r\n50 \tLogothetis, N.K. (2008) What we can do and what we cannot do with fMRI. Nature 453, 869–878\r\n\r\n51 \tDas, A. and Fiete, I.R. (2020) Systematic errors in connectivity inferred from activity in strongly recurrent networks. Nat. Neurosci. 23, \r\n\r\n52 \tStevenson, I.H. and Kording, K.P. (2011) How advances in neural recording affect data analysis. Nat. Neurosci. 14, 139–142\r\n\r\n53 \tStringer, C. et al. (2018) Spontaneous behaviors drive multidimensional , brain-wide neural activity. bioRxiv DOI: 10.1101/306019\r\n\r\n54 \tMusall, S. et al. (2018) Movement-related activity dominates cortex during sensory-guided decision making. bioRxiv DOI: 10.1101/308288\r\n\r\n55 \tCunningham, J.P. and Yu, B.M. (2014) Dimensionality reduction for large-scale neural recordings. Nat. Neurosci. 17, 1500–1509\r\n\r\n56 \tCooper, L. and Scofield, C. (1988) Mean-field theory of a neural network. Proc. Natl. … 85, 1973–1977\r\n\r\n57 \tGerstner, W. et al. (2012) Theory and simulation in neuroscience. Science (80-. ). 338, 60–65\r\n\r\n58 \tDeco, G. et al. (2008) The dynamic brain: From spiking neurons to neural masses and cortical fields. PLoS Comput. Biol. 4, \r\n\r\n59 \tPinotsis, D. et al. (2014) Neural masses and fields : modeling the dynamics of brain activity. Front. Comput. Neurosci. 8, 2013–2015\r\n\r\n60 \tHubel, D. and Wiesel, T. (1962) RECEPTIVE FIELDS,BINOCULAR INTERACTION AND FUNCTIONAL ARCHITECTURE IN THE CAT ’ S VISUAL CORTEX From the Neurophysiolojy Laboratory , Department of Pharmacology central nervous system is the great diversity of its cell types and inter- receptive fields of. J. Physiol. 160, 106–154\r\n\r\n61 \tMangalam, M. and Kelty-Stephen, D.G. (2021) Point estimates, Simpson’s paradox, and nonergodicity in biological sciences. Neurosci. Biobehav. Rev. 125, 98–107\r\n\r\n62 \tWang, Y. and Blei, D.M. (2018) The Blessings of Multiple Causes. arXiv DOI: 10.1245/s10434-012-2513-1\r\n\r\n63 \tRanganath, R. and Perotte, A. (2018) Multiple Causal Inference with Latent Confounding. arXiv at <http://arxiv.org/abs/1805.08273>\r\n\r\n64 \tSpirtes, P. et al. (2001) Causation, Prediction and Search (Adaptive Computation and Machine Learning Series), (2nd edn) MIT University Press.\r\n\r\n65 \tJonas Peters et al. (2017) Elements of causal inference: foundations and learning algorithms, MIT Press.\r\n\r\n66 \tLin, A. et al. (2022) Imaging whole-brain activity to understand behaviour. Nat. Rev. Phys. 4, 292–305\r\n\r\n67 \tJonas, E. and Kording, K.P. (2017) Could a Neuroscientist Understand a Microprocessor? PLoS Comput. Biol. 13, 1–24\r\n\r\n68 \tSaunders, J. (2019) , Correlation isn’t Connectivity. . [Online]. Available: http://jon-e.net/blog/2019/03/12/Correlation-Isnt-Connectivity/\r\n\r\n69 \tMarinescu, I.E. et al. (2018) Quasi-experimental causality in neuroscience and behavioural research. Nat. Hum. Behav. DOI: 10.1038/s41562-018-0466-5\r\n\r\n70 \tWilms, R. et al. (2021) Methods in Psychology Omitted variable bias : A threat to estimating causal relationships. 5, \r\n\r\n71 \tHart, B. et al. (2022) Neuromatch Academy: a 3-week, online summer school in computational neuroscience. J. Open Source Educ. 5, 118\r\n\r\n72 \tvan Viegen, T. et al. (2021) Neuromatch Academy: Teaching Computational Neuroscience with Global Accessibility. Trends Cogn. Sci. 25, 535–538\r\n\r\n73 \tRomei, V. et al. (2016) Causal evidence that intrinsic beta-frequency is relevant for enhanced signal propagation in the motor system as shown through rhythmic TMS. Neuroimage 126, 120–30\r\n\r\n74 \tHäusser, M. (2014) Optogenetics: The age of light. Nat. Methods 11, 1012–1014\r\n\r\n75 \tMuldoon, S.F. et al. (2016) Stimulation-Based Control of Dynamic Brain Networks. PLoS Comput. Biol. 12, \r\n\r\n76 \tSack, A.T. and Linden, D.E.J. (2003) Combining transcranial magnetic stimulation and functional imaging in cognitive brain research: possibilities and limitations. Brain Res. Brain Res. Rev. 43, 41–56\r\n\r\n77 \tO’Shea, J. et al. (2007) Functionally Specific Reorganization in Human Premotor Cortex. Neuron 54, 479–490\r\n\r\n78 \tKrakauer, J.W. et al. (2017) Neuroscience Needs Behavior: Correcting a Reductionist Bias. Neuron 93, 480–490\r\n\r\n79 \tSitaram, R. et al. (2017) Closed-loop brain training: The science of neurofeedback. Nat. Rev. Neurosci. 18, 86–100\r\n\r\n80 \tRamot, M. et al. (2017) Direct modulation of aberrant brain network connectivity through real - time NeuroFeedback. Elife 6:e28974, 1–23\r\n\r\n81 \tYamashita, A. et al. (2017) Connectivity Neurofeedback Training Can Differentially Change Functional Connectivity and Cognitive Performance. Cereb. Cortex 27, 4960–4970\r\n\r\n82 \tGolub, M.D. et al. (2018) Learning by neural reassociation. Nat. Neurosci. 21, 607–616\r\n\r\n83 \tRamot, M. and Martin, A. (2022) Closed-loop neuromodulation for studying spontaneous activity and causality. Trends Cogn. Sci. 26, 290–299\r\n\r\n84 \tMoreau, A.Q. and Dumas, G. (2021) Beyond \" Correlation vs . Causation \": Multi-brain Neuroscience needs Explanation. Trends Cogn. Sci. \r\n\r\n85 \tNovembre, G. and Iannetti, G.D. (2021) Hyperscanning Alone Cannot Prove Causality. Multibrain Stimulation Can. Trends Cogn. Sci. 25, 96–99\r\n\r\n86 \tKvamme, T.L. et al. (2022) Can neurofeedback provide evidence of direct brain-behavior causality? Neuroimage 258, 119400\r\n\r\n87 \tSorger, B. et al. (2019) Control freaks: Towards optimal selection of control conditions for fMRI neurofeedback studies. Neuroimage 186, 256–265\r\n\r\n88 \tAthalye, V.R. et al. (2018) Evidence for a Neural Law of Effect. 1029, 1024–1029\r\n\r\n89 \tBernal-Casas, D. et al. (2017) Studying Brain Circuit Function with Dynamic Causal Modeling for Optogenetic fMRI. Neuron 93, 522-532.e5\r\n\r\n90 \tBalderston, N.L. et al. (2022) Proof of concept study to develop a novel connectivity-based electric-field modelling approach for individualized targeting of transcranial magnetic stimulation treatment. Neuropsychopharmacology 47, 588–598\r\n\r\n91 \tMehler, D.M.A. (2022) Turning markers into targets – scoping neural circuits for motor neurofeedback training in Parkinson’s disease. Brain-Apparatus Commun. A J. Bacomics 1, 1–41\r\n\r\n92 \tBruno, A.M. et al. (2017) A spiral attractor network drives rhythmic locomotion. Elife 6, 1–24\r\n\r\n93 \tGordon, E.M. et al. (2017) Precision Functional Mapping of Individual Human Brains. Neuron 95, 791-807.e7\r\n\r\n94 \tGallego, J.A. et al. (2017) Multiple tasks viewed from the neural manifold: Stable control of varied behavior. bioRxiv DOI: 10.1101/176081\r\n\r\n95 \tDesowska, A. and Turner, D.L. (2019) Dynamics of brain connectivity after stroke. Rev. Neurosci. 30, 605–623\r\n\r\n96 \tMarceau, E.M. et al. (2018) Biomarker correlates of psychotherapy outcomes in borderline personality disorder: A systematic review. Neurosci. Biobehav. Rev. 94, 166–178\r\n\r\n97 \tBeijers, L. et al. (2019) Data-driven biological subtypes of depression: systematic review of biological approaches to depression subtyping. Mol. Psychiatry 24, 888–900\r\n\r\n98 \tBolt, T. et al. (2022) A parsimonious description of global functional brain organization in three spatiotemporal patterns. Nat. Neurosci. DOI: 10.1038/s41593-022-01118-1\r\n\r\n99 \tGrimsley, C. et al. (2020) Why Attention is Not Explanation : Surgical Intervention and Causal Reasoning about Neural Models. \r\n\r\n\r\n\r\n\r\n\r\n\n\nRespond ONLY with the JSON object."
  }
]